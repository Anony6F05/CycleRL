<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="CycleRL: Sim-to-Real Deep Reinforcement Learning for Robust Autonomous Bicycle Control">
  <meta name="keywords" content="Deep reinforcement learning, autonomous bicycle control, sim-to-real transfer, domain randomization, underactuated systems, lateral control">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>CycleRL: Sim-to-Real Deep Reinforcement Learning for Robust Autonomous Bicycle Control</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/bike-icon-32.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>




<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">CycleRL: Sim-to-Real Deep Reinforcement Learning for Robust Autonomous Bicycle Control</h1>
          <div class="is-size-5 publication-authors">
            <!-- Authors -->
            <span class="author-block">
              Anonymous1<sup>1</sup>,</span>
            <span class="author-block">
              Anonymous2<sup>1</sup>,</span>
            <span class="author-block">
              Anonymous3<sup>1</sup>,</span>
            <span class="author-block">
              Anonymous4<sup>1</sup>,</span>
            <span class="author-block">
              Anonymous5<sup>1</sup>,</span>
            <span class="author-block">
              Anonymous6<sup>1,*</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <!-- Affiliations -->
            <span class="author-block"><sup>1</sup>Affiliation,</span>
            <span class="author-block"><sup>*</sup>Corresponding author</a></span>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Autonomous bicycles offer a promising agile solution for urban mobility and last-mile logistics, however, conventional control strategies often struggle with their underactuated nonlinear dynamics, suffering from sensitivity to model mismatches and limited adaptability to real-world uncertainties. To address this, this paper presents CycleRL, the first sim-to-real deep reinforcement learning framework designed for robust autonomous bicycle control. Our approach trains an end-to-end neural control policy within the high-fidelity NVIDIA Isaac Sim environment, leveraging Proximal Policy Optimization (PPO) to circumvent the need for an explicit dynamics model. The framework features a composite reward function tailored for concurrent balance maintenance, velocity tracking, and steering control. Crucially, systematic domain randomization is employed to bridge the simulation-to-reality gap and facilitate direct transfer. In simulation, CycleRL achieves considerable performance, including a 99.90% balance success rate, a low steering tracking error of 1.15Â°, and a velocity tracking error of 0.18 m/s. These quantitative results, coupled with successful hardware transfer, validate DRL as an effective paradigm for autonomous bicycle control, offering superior adaptability over traditional methods. Video demonstrations are available at https://anony6f05.github.io/CycleRL/.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <video id="teaser" autoplay muted loop controls playsinline style="width: 100%; height: auto;">
            <source src="./static/videos/Demo-v6.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </div>
        <!-- <h2 class="subtitle has-text-justified"> -->
        <div class="content has-text-justified">
          <p>
            CycleRL: A robust sim-to-real Deep Reinforcement Learning framework for autonomous bicycle control. Our approach achieves direct policy transfer from high-fidelity simulation to physical hardware without explicit dynamic modeling. The video demonstrates rigorous validation across scenarios that challenge traditional control stability, ranging from handling physical variations (payload, abnormal tire pressure and varying speed) to agile maneuvering on diverse terrains like asphalt road, gravel, uneven lawn, slope, speed bump and lateral perturbations. It concludes with demonstrations of autonomous lane tracking and long-duration verification, confirming the system's exceptional reliability.
          </p>
        </div>
        <!-- </h2> -->
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>


<!-- Conclusion. -->
<!--
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Conclusion</h2>
        <div class="content has-text-justified">
          <p>
            This work presented a deep reinforcement learning framework for the robust lateral control of an autonomous bicycle. By training an end-to-end policy, our model-free approach successfully navigates the vehicle's complex, underactuated dynamics, outperforming traditional model-based controllers while bypassing the need for explicit system identification.
          </p>
          <p>
            Our key contributions include a multi-objective reward function that balances stability with tracking objectives, and a comprehensive domain randomization strategy, which proved crucial for bridging the reality gap, enabling successful zero-shot sim-to-real transfer of the learned policy. Extensive real-world experiments validated the controller's robustness across diverse operating conditions, confirming its practical viability for autonomous cycling applications, as illustrated in https://anony6f05.github.io/CycleRL/.
          </p>
          <p>
            Future work will focus on extending this framework to include vision-based navigation and multi-agent coordination. The success of our approach demonstrates the potential of model-free DRL for developing robust and intelligent mobility systems, contributing to the future of sustainable urban transportation.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
-->
<!--/ Conclusion. -->


  </div>
</section>


<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{liu2024cyclerl,
  author    = {Liu, Gelu and Wu, Zhijie and Wu, Junliang and Wang, Teng and Li, Songyuan and Zhu, Xiangwei},
  title     = {CycleRL: Sim-to-Real Deep Reinforcement Learning for Robust Autonomous Bicycle Control},
  journal   = {arXiv preprint arXiv:24xx.xxxx},
  year      = {2024},
}</code></pre>
  </div>
</section> -->


<!-- 
<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./main.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/your-repo" class="external-link">
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the source code of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>
-->

</body>
</html>
